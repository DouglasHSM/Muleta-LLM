{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/douglashsm/llm-crutch?scriptVersionId=263271673\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"fc85dc55","metadata":{"papermill":{"duration":0.002932,"end_time":"2025-09-22T06:04:48.999279","exception":false,"start_time":"2025-09-22T06:04:48.996347","status":"completed"},"tags":[]},"source":["# LLM Crutch 🤖\n","\n","> ### Tired of asking your data questions and getting... guesses? So were we.\n","\n","**A Conversational Business Intelligence Agent for the BigQuery AI Hackathon that doesn't just answer questions—it finds the truth.**\n","\n","---\n","\n","### 🚀 See it in Action!\n","\n","Before you dive in, see the magic happen first-hand.\n","\n","* **[==(https://youtu.be/3RJd8icgh6g)** (👈 Link your 2-3 minute video here)\n","* **https://muleta-llm.streamlit.app/** (👈 The ngrok link generated by your notebook)\n","\n","### 🎯 The Billion-Dollar Problem: AI's Trust Deficit\n","\n","Generative AI is a revolution. But in the world of data, it has a dirty little secret: **it lies**. Not maliciously, but it \"hallucinates,\" providing confident-sounding answers that are subtly wrong or completely fabricated. For a business relying on data for critical decisions, \"mostly right\" is 100% wrong.\n","\n","How can we harness the incredible conversational power of LLMs without inheriting their unreliability?\n","\n","### 💡 Our Solution: The \"LLM Crutch\"\n","\n","We didn't try to build a \"smarter\" AI. We built a *wiser* one.\n","\n","**LLM Crutch** is an architectural pattern where the LLM is not the oracle, but a brilliant **interpreter**. It leans on a \"crutch\"—the unshakable, high-performance foundation of **Google BigQuery**—to ensure every single answer is grounded in factual, verifiable data.\n","\n","Our agent, **QueryMaster**, embodies this philosophy. It's a conversational partner that listens to your business question, reasons about your intent, translates it into sophisticated SQL, delegates the heavy lifting to BigQuery, and presents the results beautifully. It's a `conversation-to-insight` pipeline you can actually trust.\n","\n","### ✨ Key Features\n","\n","* **🧠 True Conversational Logic:** The agent asks clarifying questions for vague prompts, just like a real data analyst.\n","* **🚀 Complex SQL on Demand:** Generates sophisticated queries with `JOIN`s, complex calculations, and aggregations from natural language.\n","* **🎨 AI-Advised Visualizations:** The LLM suggests the display format for the data (`currency`, `percentage`), creating a smarter and simpler front-end.\n","* **🔍 Full Transparency:** The exact SQL query used is always displayed, building trust in the result.\n","* **⚡ Interactive & Real-Time:** Built with Streamlit, the interface is fast and allows for a seamless analytical dialogue.\n","\n","### 🛠️ Architecture & Tech Stack\n","\n","Our \"self-contained agent\" model is designed for performance and simplicity, running entirely within a notebook environment.\n","\n","**Flow:**\n","> User → `Streamlit UI` → `Gemini API` → (Generates SQL) → `BigQuery API` → (Returns Data) → `Streamlit UI`\n","\n","**Tech Stack:**\n","* **Front-End & Orchestration:** `Streamlit`\n","* **AI Reasoning Engine:** `Google Gemini 1.5 Flash`\n","* **Data Warehouse & Processing:** `Google BigQuery`\n","* **Data Handling & Charting:** `Pandas`, `Plotly Express`\n","\n","### 🔮 Future Vision\n","\n","The \"LLM Crutch\" pattern is endlessly extensible. The next steps are clear:\n","* **Forecasting Integration:** Leverage `BigQuery ML` to answer questions about the future.\n","* **Multimodal Crutches:** Allow the agent to ingest unstructured data from the web (e.g., customer sentiment) and correlate it with hard sales data from BigQuery.\n","\n","---\n","*A project developed for the BigQuery AI Hackathon. We believe the future of data is about building smarter, more reliable systems that bridge the gap between human questions and verifiable truth.*\n","=======\n","\n"]},{"cell_type":"code","execution_count":1,"id":"9816447f","metadata":{"execution":{"iopub.execute_input":"2025-09-22T06:04:49.005384Z","iopub.status.busy":"2025-09-22T06:04:49.005069Z","iopub.status.idle":"2025-09-22T06:04:56.69437Z","shell.execute_reply":"2025-09-22T06:04:56.692652Z"},"papermill":{"duration":7.694884,"end_time":"2025-09-22T06:04:56.696534","exception":false,"start_time":"2025-09-22T06:04:49.00165","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h"]}],"source":["!pip install -q streamlit google-generativeai google-cloud-bigquery pandas db-dtypes plotly pyngrok tenacity"]},{"cell_type":"code","execution_count":2,"id":"e1424acd","metadata":{"execution":{"iopub.execute_input":"2025-09-22T06:04:56.704249Z","iopub.status.busy":"2025-09-22T06:04:56.703915Z","iopub.status.idle":"2025-09-22T06:04:56.716335Z","shell.execute_reply":"2025-09-22T06:04:56.715348Z"},"papermill":{"duration":0.018602,"end_time":"2025-09-22T06:04:56.718009","exception":false,"start_time":"2025-09-22T06:04:56.699407","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","\n","# ===============================================================\n","# app.py - FINAL SUBMISSION VERSION (CORRECTED INDENTATION)\n","# ===============================================================\n","\n","import streamlit as st\n","import google.generativeai as genai\n","from google.cloud import bigquery\n","from google.oauth2 import service_account\n","import os\n","import json\n","import pandas as pd\n","import plotly.express as px\n","from kaggle_secrets import UserSecretsClient\n","\n","# --- PAGE CONFIGURATION ---\n","st.set_page_config(\n","    page_title=\"QueryMaster AI for BigQuery\",\n","    page_icon=\"🤖\",\n","    layout=\"wide\",\n","    initial_sidebar_state=\"expanded\"\n",")\n","\n","# --- CSS TO HIDE STREAMLIT'S DEFAULT MENU AND FOOTER ---\n","hide_st_style = \"\"\"\n","            <style>\n","            #MainMenu {visibility: hidden;}\n","            footer {visibility: hidden;}\n","            </style>\n","            \"\"\"\n","st.markdown(hide_st_style, unsafe_allow_html=True)\n","\n","\n","# --- 1. SETUP AND AUTHENTICATION ---\n","try:\n","    user_secrets = UserSecretsClient()\n","    google_api_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n","    gcp_key_content = user_secrets.get_secret(\"GCP_KEY\")\n","\n","    genai.configure(api_key=google_api_key)\n","    \n","    gcp_key_json = json.loads(gcp_key_content)\n","    credentials = service_account.Credentials.from_service_account_info(gcp_key_json)\n","    client_bq = bigquery.Client(credentials=credentials, project=credentials.project_id)\n","    \n","    st.session_state.auth_success = True\n","except Exception as e:\n","    st.error(f\"Authentication Error. Please check your Kaggle Secrets. Details: {e}\")\n","    st.session_state.auth_success = False\n","    st.stop()\n","    \n","# --- SYSTEM INSTRUCTION FOR THE LLM ---\n","SYSTEM_INSTRUCTION = \"\"\"\n","You are 'QueryMaster', an AI Data Analyst specializing in the Google BigQuery 'TheLook' e-commerce dataset.\n","Your mission is to transform business questions into valid BigQuery Standard SQL queries.\n","\n","You will use the following main schema:\n","CREATE TABLE `bigquery-public-data.thelook_ecommerce.order_items` (\n","  order_id STRING, user_id STRING, product_id STRING, sale_price NUMERIC, created_at TIMESTAMP\n",");\n","CREATE TABLE `bigquery-public-data.thelook_ecommerce.products` (\n","  id STRING, cost NUMERIC, category STRING, name STRING, brand STRING, department STRING\n",");\n","CREATE TABLE `bigquery-public-data.thelook_ecommerce.users` (\n","  id STRING, email STRING, first_name STRING, last_name STRING\n",");\n","\n","Your response MUST be a valid JSON object.\n","The JSON must have the keys \"action\" and \"content\".\n","- For vague questions, use: {\"action\": \"CLARIFY\", \"content\": \"Your clarification question here.\"}\n","- To generate SQL, use: {\"action\": \"EXECUTE\", \"content\": \"Your SQL query here.\", \"display_format\": \"...\"}\n","\n","Possible values for \"display_format\": \"currency_usd\", \"percentage\", \"number\".\n","\n","IMPORTANT: To group data by month and year from a TIMESTAMP column like 'created_at', use the function FORMAT_TIMESTAMP('%Y-%m', created_at). NEVER use the strftime function.\n","\"\"\"\n","\n","# Continue only if authentication was successful\n","if 'auth_success' in st.session_state and st.session_state.auth_success:\n","    model = genai.GenerativeModel(\"gemini-1.5-flash\", system_instruction=SYSTEM_INSTRUCTION)\n","    \n","    # --- 2. BACKEND LOGIC (HELPER FUNCTIONS) ---\n","\n","    def clean_json_from_string(text):\n","        \"\"\"Extracts a JSON string from within a Markdown code block.\"\"\"\n","        start_index = text.find('{')\n","        end_index = text.rfind('}')\n","        if start_index != -1 and end_index != -1:\n","            return text[start_index:end_index+1]\n","        return text\n","\n","    # --- THIS FUNCTION IS NOW CORRECTED ---\n","    @st.cache_data\n","    def get_assistant_response(user_prompt, history_tuple):\n","        \"\"\"\n","        This function encapsulates the backend logic. It calls the LLM and, if required, BigQuery.\n","        \"\"\"\n","        history = [json.loads(item) for item in history_tuple]\n","        try:\n","            chat_session = model.start_chat(history=history)\n","            response = chat_session.send_message(user_prompt)\n","            cleaned_text = clean_json_from_string(response.text)\n","            \n","            response_json = json.loads(cleaned_text)\n","            action = response_json.get(\"action\")\n","\n","            if action != \"EXECUTE\":\n","                return response_json # If the action is CLARIFY, return immediately.\n","\n","            # If the action IS EXECUTE, the code continues below.\n","            sql_query = response_json.get(\"content\")\n","            display_format = response_json.get(\"display_format\", \"number\")\n","            \n","            query_job = client_bq.query(sql_query)\n","            df_results = query_job.to_dataframe()\n","            \n","            return {\n","                \"action\": \"DATA\", \n","                \"content\": df_results.to_dict('records'),\n","                \"query_used\": sql_query, \n","                \"display_format\": display_format \n","            }\n","\n","        except json.JSONDecodeError:\n","            return {\"action\": \"ERROR\", \"content\": f\"The model responded in an unexpected format: '{response.text}'\"}\n","        except Exception as e:\n","            return {\"action\": \"ERROR\", \"content\": f\"An error occurred: {e}\"}\n","\n","    def process_and_display_prompt(prompt):\n","        \"\"\"Processes a prompt from the chat input or a button and displays the results in the UI.\"\"\"\n","        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n","        with st.chat_message(\"user\"):\n","            st.markdown(prompt)\n","\n","        with st.chat_message(\"assistant\"):\n","            with st.spinner(\"Analyzing data...\"):\n","                history_for_cache = tuple(json.dumps(item) for item in st.session_state.history_for_api)\n","                response_data = get_assistant_response(prompt, history_for_cache)\n","                action = response_data.get(\"action\")\n","                if action == \"CLARIFY\":\n","                    message_content = response_data[\"content\"]\n","                    st.markdown(message_content)\n","                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": message_content})\n","                elif action == \"DATA\":\n","                    data_content = response_data[\"content\"]\n","                    display_format = response_data.get(\"display_format\", \"number\")\n","                    if not data_content:\n","                        st.warning(\"The query returned no results.\")\n","                    else:\n","                        df = pd.DataFrame(data_content)\n","                        if df.shape[0] == 1 and df.shape[1] == 1:\n","                            st.markdown(\"#### Key Metric\")\n","                            kpi_value = df.iloc[0, 0]\n","                            kpi_label = df.columns[0].replace(\"_\", \" \").title()\n","                            if display_format == \"percentage\": formatted_value = f\"{kpi_value:,.2f}%\"\n","                            elif display_format == \"currency_usd\": formatted_value = f\"${kpi_value:,.2f}\"\n","                            else: formatted_value = f\"{kpi_value:,}\"\n","                            st.metric(label=kpi_label, value=formatted_value)\n","                        else:\n","                            col1, col2 = st.columns([1, 1.2])\n","                            with col1:\n","                                st.markdown(\"#### Detailed Data\")\n","                                st.dataframe(df)\n","                            with col2:\n","                                st.markdown(\"#### Chart\")\n","                                try:\n","                                    x_axis, y_axis = df.columns[0], df.columns[1]\n","                                    fig = px.bar(df, x=x_axis, y=y_axis, title=f'{y_axis.replace(\"_\", \" \").title()} by {x_axis.replace(\"_\", \" \").title()}', template=\"seaborn\")\n","                                    st.plotly_chart(fig, use_container_width=True)\n","                                except Exception:\n","                                    st.warning(\"Could not generate a chart for this data.\")\n","                    with st.expander(\"View the generated SQL query\"):\n","                        st.code(response_data[\"query_used\"], language=\"sql\")\n","                    message_content = \"[Displaying data and charts]\"\n","                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": message_content})\n","                else:\n","                    message_content = f\"An error occurred: {response_data.get('content')}\"\n","                    st.error(message_content)\n","                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": message_content})\n","        st.session_state.history_for_api.append({\"role\": \"user\", \"parts\": [prompt]})\n","        st.session_state.history_for_api.append({\"role\": \"model\", \"parts\": [json.dumps(response_data)]})\n","\n","    # --- 3. STREAMLIT UI ---\n","    st.title(\"LLM Crutch 🤖: Your Data Analysis Assistant\")\n","    st.caption(\"A project for the Kaggle BigQuery AI Hackathon by Douglas Menezes\")\n","    st.sidebar.title(\"Analysis Suggestions 💡\")\n","    st.sidebar.markdown(\"Click a button to ask a sample question!\")\n","    if st.sidebar.button(\"📊 Monthly Profit (Chart)\"): process_and_display_prompt(\"Show me the monthly profit evolution for the year 2023.\")\n","    if st.sidebar.button(\"🏆 Top 5 Profitable Brands (Table)\"): process_and_display_prompt(\"What are the 5 most profitable brands?\")\n","    if st.sidebar.button(\"💰 Annual Growth (KPI %)\") : process_and_display_prompt(\"What was the percentage revenue growth between 2022 and 2023?\")\n","    if st.sidebar.button(\"🤯 Top Spenders Analysis (Complex JOIN)\"): process_and_display_prompt(\"List the top 3 users (with their emails) who spent the most on 'Jeans' products.\")\n","    st.sidebar.markdown(\"---\")\n","    st.sidebar.title(\"Controls\")\n","    if st.sidebar.button(\"🧹 Clear Conversation\"):\n","        st.session_state.messages, st.session_state.history_for_api = [], []\n","        st.rerun()\n","    st.sidebar.markdown(\"---\")\n","    st.sidebar.info(\"**About:** 'LLM Crutch' is a conversational BI tool using Google's Gemini AI to translate natural language into SQL queries, executed on BigQuery.\")\n","    if \"messages\" not in st.session_state:\n","        st.session_state.messages, st.session_state.history_for_api = [], []\n","    for message in st.session_state.messages:\n","        with st.chat_message(message[\"role\"]): st.markdown(message[\"content\"])\n","    if prompt := st.chat_input(\"Ask your own question about the data...\"):\n","        process_and_display_prompt(prompt)"]},{"cell_type":"code","execution_count":3,"id":"c4a0ed68","metadata":{"execution":{"iopub.execute_input":"2025-09-22T06:04:56.724532Z","iopub.status.busy":"2025-09-22T06:04:56.724233Z","iopub.status.idle":"2025-09-22T06:05:02.75961Z","shell.execute_reply":"2025-09-22T06:05:02.758514Z"},"papermill":{"duration":6.040631,"end_time":"2025-09-22T06:05:02.761388","exception":false,"start_time":"2025-09-22T06:04:56.720757","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Authtoken do ngrok configurado!\n","✅ Servidor Streamlit iniciado em segundo plano!\n","\n","Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n","\n","\n","  You can now view your Streamlit app in your browser.\n","\n","  Local URL: http://localhost:8501\n","  Network URL: http://172.19.2.2:8501\n","  External URL: http://35.227.15.19:8501\n","\n","❌ Erro ao criar o túnel do ngrok. Detalhes: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.\n"]},{"name":"stderr","output_type":"stream","text":["ERROR:  authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\n","ERROR:  You can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\n","ERROR:  Read more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\n","ERROR:  You can view your current agent sessions in the dashboard:\n","ERROR:  https://dashboard.ngrok.com/agents\r\n","ERROR:  \r\n","ERROR:  ERR_NGROK_108\r\n","ERROR:  https://ngrok.com/docs/errors/err_ngrok_108\r\n","ERROR:  \n"]}],"source":["# ===============================================================\n","# CÉLULA LANÇADORA FINAL )\n","# ===============================================================\n","\n","from pyngrok import ngrok\n","from kaggle_secrets import UserSecretsClient\n","import subprocess\n","import threading\n","import time\n","\n","# --- 1. CONFIGURAR O NGROK ---\n","try:\n","    user_secrets = UserSecretsClient()\n","    ngrok_token = user_secrets.get_secret(\"NGROK_AUTHTOKEN\")\n","    ngrok.set_auth_token(ngrok_token)\n","    print(\"✅ Authtoken do ngrok configurado!\")\n","except Exception as e:\n","    print(f\"❌ Erro ao configurar o ngrok. Verifique seus Secrets. Detalhes: {e}\")\n","\n","# --- 2. FUNÇÃO PARA INICIAR O STREAMLIT EM OUTRA THREAD ---\n","def run_streamlit():\n","    # Usamos subprocess para chamar o comando do streamlit\n","    command = [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.headless=true\"]\n","    process = subprocess.Popen(command)\n","    process.wait()\n","\n","# --- 3. INICIAR TUDO ---\n","# Criamos e iniciamos a \"thread\" para o Streamlit.\n","# Isso faz com que o Streamlit rode em paralelo, sem travar o notebook.\n","streamlit_thread = threading.Thread(target=run_streamlit)\n","streamlit_thread.start()\n","print(\"✅ Servidor Streamlit iniciado em segundo plano!\")\n","\n","# Damos um pequeno tempo para o servidor Streamlit começar a esquentar\n","time.sleep(5)\n","\n","# Agora que o Streamlit está rodando, iniciamos o túnel do ngrok\n","try:\n","    public_url = ngrok.connect(8501)\n","    print(\"---\")\n","    print(\"🚀 Sua aplicação está no ar!\")\n","    print(f\"Clique neste link para acessar: {public_url}\")\n","    print(\"---\")\n","except Exception as e:\n","    print(f\"❌ Erro ao criar o túnel do ngrok. Detalhes: {e}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMbLyWTS7+wVEE4U9EGAJlt","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":13391012,"sourceId":110281,"sourceType":"competition"}],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":20.174798,"end_time":"2025-09-22T06:05:03.291341","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-22T06:04:43.116543","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}